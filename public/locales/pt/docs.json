{
  "title": "Documentação",
  "subtitle": "Aprenda a criar APIs de IA estruturadas com o {{appName}}",
  "seo": {
    "gettingStarted": "Saiba como começar com o {{appName}}. Crie projetos, configure provedores de LLM e crie seu primeiro ponto de acesso de IA estruturada.",
    "concepts": "Compreenda conceitos centrais: pontos de extremidade, esquemas JSON, contexto do sistema e caminhos de organização para criar APIs de IA estruturadas.",
    "apiReference": "Referência completa da API para {{appName}. Pontos finais REST, formatos de solicitação, estruturas de resposta e exemplos de código.",
    "default": "Documentação do {{appName}}. Aprenda a criar APIs REST confiáveis com poder de IA e validação com JSON Schema."
  },
  "howTo": {
    "name": "Como criar um ponto de acesso estruturado para LLM",
    "description": "Guia passo a passo para criar seu primeiro ponto de acesso de API de IA estruturada com {{appName}}",
    "steps": {
      "createProject": {
        "name": "Criar um Projeto",
        "text": "Comece criando um novo projeto no seu painel para organizar suas extremidades."
      },
      "addProvider": {
        "name": "Adicionar um provedor de LLM",
        "text": "Configure sua chave de API para OpenAI, Anthropic, Google Gemini ou um servidor personalizado de LM."
      },
      "defineEndpoint": {
        "name": "Defina um ponto de extremidade",
        "text": "Crie um endpoint com instruções, contexto e esquema de saída JSON."
      },
      "testEndpoint": {
        "name": "Teste seu endpoint",
        "text": "Use o testador integrado para verificar se sua ponta de acesso retorna dados estruturados."
      },
      "integrate": {
        "name": "Integrar",
        "text": "Chame seu ponto de extremidade a partir de sua aplicação usando solicitações GET ou POST simples."
      }
    }
  },
  "nav": {
    "gettingStarted": "Começando",
    "concepts": "Conceitos Principais",
    "apiReference": "Referência da API"
  },
  "gettingStarted": {
    "title": "Começando",
    "steps": [
      {
        "title": "Criar um Projeto",
        "description": "Comece criando um novo projeto no seu painel. Dê a ele um nome e uma descrição opcional."
      },
      {
        "title": "Adicione um Provedor",
        "description": "Configure sua chave de API para OpenAI, Anthropic, Google Gemini ou um servidor de LM personalizado."
      },
      {
        "title": "Defina um ponto de extremidade",
        "description": "Crie um ponto de extremidade com esquema de saída JSON, instruções e contexto. Conecte-o à sua chave LLM."
      },
      {
        "title": "Defina o caminho da sua organização",
        "description": "Configure um caminho de organização personalizado nas configurações, ou use o caminho padrão gerado automaticamente."
      },
      {
        "title": "Integrar",
        "description": "Chame seu ponto de extremidade a partir de sua aplicação usando solicitações GET ou POST simples."
      }
    ]
  },
  "concepts": {
    "title": "Conceitos Principais",
    "endpoints": {
      "title": "Pontos de extremidade",
      "description": "Cada endpoint é configurado com:",
      "items": [
        {
          "name": "Esquema de Entrada",
          "description": "Esquema JSON que define a estrutura esperada dos dados de entrada (opcional)"
        },
        {
          "name": "Esquema de saída",
          "description": "Esquema JSON que define a estrutura da resposta da LLM"
        },
        {
          "name": "Instruções",
          "description": "Instruções específicas sobre o que o modelo de linguagem deverá fazer com a entrada"
        },
        {
          "name": "Contexto",
          "description": "Contexto do sistema/prompt para orientar o comportamento do modelo de linguagem"
        },
        {
          "name": "Método HTTP",
          "description": "GET (parâmetros de consulta) ou POST (corpo JSON) para dados de entrada"
        }
      ]
    },
    "inputSchema": {
      "title": "Esquema de Entrada",
      "description": "Defina a estrutura esperada dos dados de entrada usando JSON Schema. Quando os chamadores enviarem dados para seu ponto de extremidade, {{appName}} validará esses dados com base na sua schema de entrada antes do processamento. Isso garante que sua LLM receba dados bem formados e forneça mensagens de erro claras para solicitações inválidas."
    },
    "outputSchema": {
      "title": "Esquema de saída",
      "description": "Defina a estrutura de saídas de LLM usando JSON Schema. {{appName}} instrui o LLM a retornar dados que correspondam ao seu esquema e valida a resposta, repetindo automaticamente se a saída não for compatível."
    },
    "context": {
      "title": "Contexto do sistema",
      "description": "Forneça prompts do sistema para orientar o comportamento do LLM. Este contexto é incluído em cada solicitação ao seu ponto de acesso."
    },
    "organizationPath": {
      "title": "Caminho da Organização",
      "description": "O caminho da sua organização é usado em URLs da API. Por padrão, é formado pelos primeiros 8 caracteres do seu ID de usuário, mas você pode personalizá-lo nas configurações."
    }
  },
  "apiReference": {
    "title": "Referência da API",
    "description": "Acesse seus endpoints por meio da API REST. Todos os endpoints de IA são públicos e não exigem autenticação.",
    "endpoints": {
      "main": {
        "title": "Executar ponto de extremidade",
        "get": "GET /api/v1/ia/:orgPath/:projectName/:endpointName",
        "post": "POST /api/v1/ia/:orgPath/:projectName/:endpointName",
        "description": "Executa o ponto de extremidade e retorna a resposta da LLM com métricas de uso."
      },
      "prompt": {
        "title": "Obter apenas o prompt",
        "get": "GET /api/v1/ia/:orgPath/:projectName/:endpointName/prompt",
        "post": "POST /api/v1/ia/:orgPath/:projectName/:endpointName/prompt",
        "description": "Retorna o prompt gerado sem chamar o LLM. Útil para depuração."
      }
    },
    "response": {
      "title": "Formato de Resposta",
      "description": "Respostas bem-sucedidas incluem a saída da LLM e estatísticas de uso:",
      "fields": [
        {
          "name": "saída",
          "description": "A resposta estruturada da LLM que corresponde ao seu esquema de saída"
        },
        {
          "name": "usage.tokens_input",
          "description": "Número de tokens de entrada utilizados"
        },
        {
          "name": "usage.tokens_output",
          "description": "Número de tokens de saída gerados"
        },
        {
          "name": "latência_ms_de_uso",
          "description": "Latência do pedido em milissegundos"
        },
        {
          "name": "uso.custo_estimado_em_centavos",
          "description": "Custo estimado em centavos"
        }
      ]
    },
    "example": {
      "requestTitle": "Solicitação de exemplo",
      "responseTitle": "Resposta de exemplo"
    }
  }
}